---
title:       "Naive Bayes（朴素贝叶斯）"
subtitle:    ""
description: "Naive Bayes（朴素贝叶斯）"
date:        "2025-01-21T04:26:43+08:00"
author:      "杨子逸"
image:       ""
tags:        ["程序", "算法", "机器学习"]
categories:  ["Tech"]
draft:       false
---


# 介绍
Naive Bayes（朴素贝叶斯）是一种基于贝叶斯定理的概率分类算法，广泛用于文本分类、垃圾邮件检测和情感分析等任务。它简单高效，特别适合高维数据。
- 条件独立性假设： 特征之间相互独立（即每个特征对结果的影响是独立的）。尽管这个假设在实际中很少完全成立，但在很多场景下它的性能仍然很好。
- 类别条件概率： 对每个类别，计算给定特征的概率，然后选择概率最大的类别。

# 贝叶斯核心原理
贝叶斯定理的核心公式如下：

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

其中：
- \( P(A|B) \) 是在事件 B 发生的情况下事件 A 发生的概率（后验概率）。
- \( P(B|A) \) 是在事件 A 发生的情况下事件 B 发生的概率（似然）。
- \( P(A) \) 是事件 A 发生的概率（先验概率）。
- \( P(B) \) 是事件 B 发生的概率（边缘概率）。

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# 示例数据
X_train = ["This is a good movie", "I hated this movie", "Amazing film, I loved it", 
           "Terrible movie, not recommended", "Excellent acting and great story"]
y_train = [1, 0, 1, 0, 1]  # 1: 正面评价, 0: 负面评价

X_test = ["Good movie with great acting", "I hated the story"]

# 文本向量化
vectorizer = CountVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# 训练朴素贝叶斯分类器
nb = MultinomialNB()
nb.fit(X_train_vec, y_train)

# 预测
predictions = nb.predict(X_test_vec)
print("预测结果:", predictions)  # 输出 [1, 0]
```

# 现实例子1(牛肉检测)
假设我们有一个检测牛排是否是马肉的测试。我们知道以下信息：

- 在所有牛排中，5%是马肉（**先验概率** \( P(\text{马肉}) = 0.05 \)）。
- 测试的准确率是98%，即如果牛排是马肉，测试有98%的概率会检测出马肉（**似然** \( P(\text{测试阳性}|\text{马肉}) = 0.98 \)）。
- 如果牛排不是马肉，测试有10%的概率会误报为马肉（**假阳性率** \( P(\text{测试阳性}|\text{非马肉}) = 0.10 \)）。

我们想知道，如果测试结果是阳性，那么牛排是马肉的概率是多少（**后验概率** \( P(\text{马肉}|\text{测试阳性}) \)）。

根据贝叶斯定理，我们可以计算：

$$
P(\text{马肉}|\text{测试阳性}) = \frac{P(\text{测试阳性}|\text{马肉}) \cdot P(\text{马肉})}{P(\text{测试阳性})}
$$

其中，\( P(\text{测试阳性}) \) 可以通过全概率公式计算：

$$
P(\text{测试阳性}) = P(\text{测试阳性}|\text{马肉}) \cdot P(\text{马肉}) + P(\text{测试阳性}|\text{非马肉}) \cdot P(\text{非马肉})
$$

代入已知值：

$$
P(\text{测试阳性}) = 0.98 \cdot 0.05 + 0.10 \cdot 0.95 = 0.049 + 0.095 = 0.144
$$

因此：

$$
P(\text{马肉}|\text{测试阳性}) = \frac{0.98 \cdot 0.05}{0.144} \approx 0.3403
$$

所以，即使测试结果是阳性，牛排实际上是马肉的概率只有约34.03%。

# 贝叶斯分类器
贝叶斯分类器是一种基于贝叶斯定理的分类算法。其核心思想是通过计算给定特征下各类别的概率，选择概率最大的类别作为预测结果。贝叶斯分类器的核心公式如下：

$$
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
$$

其中：
- \( P(C|X) \) 是在给定特征 X 的情况下类别 C 的概率（后验概率）。
- \( P(X|C) \) 是在类别 C 下特征 X 出现的概率（似然）。
- \( P(C) \) 是类别 C 的先验概率。
- \( P(X) \) 是特征 X 出现的概率（边缘概率）。

贝叶斯分类器通常假设特征之间相互独立（即条件独立性假设），这使得计算变得更加简单。尽管这一假设在实际中很少完全成立，但在很多场景下，贝叶斯分类器仍然表现良好。

贝叶斯分类器的优点包括：
- 简单高效，适用于高维数据。
- 对小规模数据集表现良好。
- 易于实现和解释。

常见的贝叶斯分类器包括朴素贝叶斯分类器、多项式贝叶斯分类器和高斯贝叶斯分类器等。


# 实例2(垃圾邮件检测)
假设我们有一个垃圾邮件检测系统。我们知道以下信息：

- 在所有邮件中，20%是垃圾邮件（**先验概率** \( P(\text{垃圾邮件}) = 0.20 \)）。
- 如果邮件是垃圾邮件，系统有90%的概率会检测出垃圾邮件（**似然** \( P(\text{检测阳性}|\text{垃圾邮件}) = 0.90 \)）。
- 如果邮件不是垃圾邮件，系统有5%的概率会误报为垃圾邮件（**假阳性率** \( P(\text{检测阳性}|\text{非垃圾邮件}) = 0.05 \)）。

我们想知道，如果系统检测结果是阳性，那么邮件是垃圾邮件的概率是多少（**后验概率** \( P(\text{垃圾邮件}|\text{检测阳性}) \)）。

根据贝叶斯定理，我们可以计算：

$$
P(\text{垃圾邮件}|\text{检测阳性}) = \frac{P(\text{检测阳性}|\text{垃圾邮件}) \cdot P(\text{垃圾邮件})}{P(\text{检测阳性})}
$$

其中，\( P(\text{检测阳性}) \) 可以通过全概率公式计算：

$$
P(\text{检测阳性}) = P(\text{检测阳性}|\text{垃圾邮件}) \cdot P(\text{垃圾邮件}) + P(\text{检测阳性}|\text{非垃圾邮件}) \cdot P(\text{非垃圾邮件})
$$

代入已知值：

$$
P(\text{检测阳性}) = 0.90 \cdot 0.20 + 0.05 \cdot 0.80 = 0.18 + 0.04 = 0.22
$$

因此：

$$
P(\text{垃圾邮件}|\text{检测阳性}) = \frac{0.90 \cdot 0.20}{0.22} \approx 0.8182
$$

所以，如果系统检测结果是阳性，邮件实际上是垃圾邮件的概率约为81.82%。

如：
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# 数据集
emails = [
    "cheap pharmaceuticals available",  # 垃圾邮件
    "enlargement drugs cheap price",   # 垃圾邮件
    "meeting schedule for tomorrow",   # 正常邮件
    "pharmaceuticals free trial now",  # 垃圾邮件
    "project deadline is next week"    # 正常邮件
]
labels = [1, 1, 0, 1, 0]  # 1: 垃圾邮件, 0: 正常邮件

# 文本向量化
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(emails)

# 朴素贝叶斯分类器
nb = MultinomialNB()
nb.fit(X, labels)

# 测试新邮件
test_emails = ["cheap drugs trial", "schedule meeting tomorrow"]
X_test = vectorizer.transform(test_emails)
predictions = nb.predict(X_test)

print("预测结果:", predictions)  # [1, 0]
```

**高效处理高维数据**
邮件文本往往包含数千甚至数万个特征（如单词）。朴素贝叶斯的条件独立性假设简化了计算，使其能够高效处理高维数据。

**简单且效果好**
- 垃圾邮件过滤需要快速处理大量邮件。
- 朴素贝叶斯分类器简单易用，训练和预测的计算代价低，且在文本分类任务中通常表现良好。

**适应不平衡数据**
垃圾邮件与正常邮件的比例可能极不平衡（垃圾邮件通常占比更高），贝叶斯分类器可以很好地适应这种不平衡数据。

**处理稀疏数据**
- 邮件中每封邮件的单词数量相对于整个词汇表来说是稀疏的。
- 朴素贝叶斯模型的计算不依赖联合分布，因此能够轻松处理这种稀疏数据。


# 贝叶斯的优缺点
优点：
- 高效：训练和预测速度快，适合高维数据（如文本分类）。
- 鲁棒性：在样本不足的情况下表现良好。
- 适合离散数据：特别适合文本数据，如词频分析。
缺点：
- 条件独立性假设：特征往往不是独立的，可能影响分类效果。
- 零概率问题：如果某特征值在训练集中从未出现，概率会变为 0（可以用拉普拉斯平滑解决）。